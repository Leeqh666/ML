{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***词向量技术(Word2Vec)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据导入\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news = fetch_20newsgroups(subset = 'all')\n",
    "X,y = news.data,news.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk,re\n",
    "\n",
    "def news_to_sentences(news):\n",
    "    news_text = BeautifulSoup(news).get_text()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(news_text)\n",
    "    sentences = []\n",
    "    for sent in raw_sentences:\n",
    "        sentences.append(re.sub('[^a-zA-Z]',' ',sent.lower().strip()).split())\n",
    "    return sentences\n",
    "#将长篇新闻文本中的句子剥离\n",
    "sentences = []\n",
    "\n",
    "for x in X:\n",
    "    sentences += news_to_sentences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'from', u'mamatha', u'devineni', u'ratnam', u'subject', u'pens', u'fans', u'reactions', u'organization', u'post', u'office', u'carnegie', u'mellon', u'pittsburgh', u'pa', u'lines', u'nntp', u'posting', u'host', u'po', u'andrew', u'cmu', u'edu', u'i', u'am', u'sure', u'some', u'bashers', u'of', u'pens', u'fans', u'are', u'pretty', u'confused', u'about', u'the', u'lack', u'of', u'any', u'kind', u'of', u'posts', u'about', u'the', u'recent', u'pens', u'massacre', u'of', u'the', u'devils'], [u'actually', u'i', u'am', u'bit', u'puzzled', u'too', u'and', u'a', u'bit', u'relieved']]\n"
     ]
    }
   ],
   "source": [
    "print sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "#词向量维度\n",
    "num_features = 300\n",
    "#被考虑词汇最小频度\n",
    "min_word_count = 20\n",
    "#使用CPU数量\n",
    "num_workers = 2\n",
    "#定义训练词向量的上下文的窗口大小\n",
    "context = 5\n",
    "downsampling = 1e-3\n",
    "\n",
    "model = word2vec.Word2Vec(sentences,workers = num_workers,size = num_features,min_count = min_word_count\n",
    "                         ,window = context,sample = downsampling)\n",
    "model.init_sims(replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'mail', 0.7413821220397949), (u'contact', 0.6773474216461182), (u'sas', 0.6681387424468994), (u'address', 0.6597812175750732), (u'mailed', 0.6453138589859009), (u'replies', 0.6424853801727295), (u'request', 0.6345109343528748), (u'send', 0.6190707683563232), (u'compuserve', 0.6185154318809509), (u'internet', 0.5937801599502563)]\n",
      "[(u'afternoon', 0.8044012784957886), (u'weekend', 0.782137393951416), (u'evening', 0.7475570440292358), (u'night', 0.7146547436714172), (u'friday', 0.6941074132919312), (u'saturday', 0.6927902698516846), (u'sunday', 0.6782878041267395), (u'summer', 0.6421786546707153), (u'thursday', 0.632861852645874), (u'yesterday', 0.627538800239563)]\n"
     ]
    }
   ],
   "source": [
    "#利用训练好的模型寻找相似词汇\n",
    "print model.wv.most_similar('email')\n",
    "print model.wv.most_similar('morning')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Py2]",
   "language": "python",
   "name": "conda-env-Py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
